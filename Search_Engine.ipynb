{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Engine\n",
    "\n",
    "In this file, we walk through the process of creating a search engine with an inverted index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from path import Path\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from math import log\n",
    "import string\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>character</th>\n",
       "      <th>quote</th>\n",
       "      <th>scene</th>\n",
       "      <th>location</th>\n",
       "      <th>view</th>\n",
       "      <th>episode</th>\n",
       "      <th>date</th>\n",
       "      <th>series</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>141466</th>\n",
       "      <td>198</td>\n",
       "      <td>WORF</td>\n",
       "      <td>Unidentified craft Sector four to Sector four ...</td>\n",
       "      <td>35   INT. MAIN BRIDGE</td>\n",
       "      <td>MAIN BRIDGE</td>\n",
       "      <td>INT.</td>\n",
       "      <td>The Outrageous Okona</td>\n",
       "      <td>1988-10-04</td>\n",
       "      <td>The Next Generation</td>\n",
       "      <td>130.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>441</td>\n",
       "      <td>ROM</td>\n",
       "      <td>I wasn't working.</td>\n",
       "      <td>70   INT. QUARK'S</td>\n",
       "      <td>QUARK'S</td>\n",
       "      <td>INT.</td>\n",
       "      <td>STAR TREK:  DEEP SPACE NINE</td>\n",
       "      <td>1996-08-29</td>\n",
       "      <td>Deep Space Nine</td>\n",
       "      <td>504.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75689</th>\n",
       "      <td>434</td>\n",
       "      <td>WORF</td>\n",
       "      <td>Very well. You may cross the border. But only ...</td>\n",
       "      <td>57   INT. MEDICAL SHIP - BRIDGE - FUTURE (OPTI...</td>\n",
       "      <td>MEDICAL SHIP - BRIDGE - FUTURE</td>\n",
       "      <td>INT.</td>\n",
       "      <td>All Good Things...</td>\n",
       "      <td>1994-03-10</td>\n",
       "      <td>The Next Generation</td>\n",
       "      <td>277.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66089</th>\n",
       "      <td>231</td>\n",
       "      <td>QUARK</td>\n",
       "      <td>Account number CJ57436.</td>\n",
       "      <td>22   INT. SECURITY OFFICE</td>\n",
       "      <td>SECURITY OFFICE</td>\n",
       "      <td>INT.</td>\n",
       "      <td>Who Mourns for Morn?</td>\n",
       "      <td>1997-10-28</td>\n",
       "      <td>Deep Space Nine</td>\n",
       "      <td>536.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55570</th>\n",
       "      <td>284</td>\n",
       "      <td>ISHKA</td>\n",
       "      <td>He said I didn't really love him... that I was...</td>\n",
       "      <td>25   EXT. ISHKA'S HOUSE</td>\n",
       "      <td>ISHKA'S HOUSE</td>\n",
       "      <td>EXT.</td>\n",
       "      <td>Ferengi Love Songs</td>\n",
       "      <td>1997-01-30</td>\n",
       "      <td>Deep Space Nine</td>\n",
       "      <td>518.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        index character                                              quote  \\\n",
       "141466    198      WORF  Unidentified craft Sector four to Sector four ...   \n",
       "441       441       ROM                                  I wasn't working.   \n",
       "75689     434      WORF  Very well. You may cross the border. But only ...   \n",
       "66089     231     QUARK                            Account number CJ57436.   \n",
       "55570     284     ISHKA  He said I didn't really love him... that I was...   \n",
       "\n",
       "                                                    scene  \\\n",
       "141466                              35   INT. MAIN BRIDGE   \n",
       "441                                     70   INT. QUARK'S   \n",
       "75689   57   INT. MEDICAL SHIP - BRIDGE - FUTURE (OPTI...   \n",
       "66089                           22   INT. SECURITY OFFICE   \n",
       "55570                             25   EXT. ISHKA'S HOUSE   \n",
       "\n",
       "                              location  view                      episode  \\\n",
       "141466                     MAIN BRIDGE  INT.         The Outrageous Okona   \n",
       "441                            QUARK'S  INT.  STAR TREK:  DEEP SPACE NINE   \n",
       "75689   MEDICAL SHIP - BRIDGE - FUTURE  INT.          All Good Things...    \n",
       "66089                  SECURITY OFFICE  INT.         Who Mourns for Morn?   \n",
       "55570                    ISHKA'S HOUSE  EXT.           Ferengi Love Songs   \n",
       "\n",
       "              date               series     file  \n",
       "141466  1988-10-04  The Next Generation  130.txt  \n",
       "441     1996-08-29      Deep Space Nine  504.txt  \n",
       "75689   1994-03-10  The Next Generation  277.txt  \n",
       "66089   1997-10-28      Deep Space Nine  536.txt  \n",
       "55570   1997-01-30      Deep Space Nine  518.txt  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start by loading in the data\n",
    "complete = pd.read_csv('https://scmcqueen.github.io/StarTrekScriptData/complete_data.csv')\n",
    "# Rename columns\n",
    "complete.columns = ['index', 'character', 'quote', 'scene', 'location', 'view',\n",
    "       'episode', 'date', 'series', 'file']\n",
    "# Clean up Character & Quote\n",
    "complete['character'] = complete['character'].apply(lambda text: \" \".join(str(text).split()))\n",
    "complete['quote']=complete['quote'].apply(lambda text: \" \".join(text.split()))\n",
    "# Show sample of data\n",
    "complete.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: POTENTIALLY FIX CHARACTER NAME TOO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data loaded in, we may want to define some functions that we will use when creating the search engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_string(input_string: str) -> str:\n",
    "    '''This function processes a string by removing punctuation,\n",
    "    making text lowercase, and getting rid of extra spaces\n",
    "\n",
    "    For example:\n",
    "        \"Hello,  HI!!! How are     you?\"\n",
    "    becomes\n",
    "        \"hello hi how are you\"\n",
    "    '''\n",
    "    translation_table = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "    string_without_punc = input_string.translate(translation_table)\n",
    "    string_without_double_spaces = ' '.join(string_without_punc.split())\n",
    "    return string_without_double_spaces.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAY WANT TO ALSO LEMMATIZE & DROP STOP WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_url_scores(old: dict[str, float], new: dict[str, float]):\n",
    "    '''This function adds two dictionaries together'''\n",
    "    for url, score in new.items():\n",
    "        if url in old:\n",
    "            old[url] += score\n",
    "        else:\n",
    "            old[url] = score\n",
    "    return old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a Search Engine object with an inverted index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class search_engine:\n",
    "    '''This class creates a search engine object'''\n",
    "    def __init__(self, index:dict[str, dict[str, int]]=None, docs: dict[str, str]=None,\n",
    "        original_docs: dict[str, str]=None,k1:float=1.5,b:float=0.75,\n",
    "        name:str='Default Search Engine',full_data: pd.DataFrame=None):\n",
    "        '''\n",
    "        Instantiate an instance of the search engine class.\n",
    "\n",
    "        Input:\n",
    "            index: dict[str, dict[int,int]], the inverted index\n",
    "            docs: dict[int, str], key is the id of the quote and value is the quote text\n",
    "            original_docs:\n",
    "            k1: float, k1 constant to use for bm25\n",
    "            b: float, b constant to use for bm25\n",
    "            name: string, name used for the search engine instance\n",
    "            full_data:  pd.DataFrame, the original dataset used\n",
    "\n",
    "        Output:\n",
    "            search engine!\n",
    "        '''\n",
    "        # set index\n",
    "        if index is None: self.index = defaultdict(lambda: defaultdict(int))\n",
    "        else: self.index = index\n",
    "        # set docs\n",
    "        if docs is None: self.docs = {}\n",
    "        else: self.docs = docs\n",
    "        # set original docs\n",
    "        if original_docs is None: self.original_docs = {}\n",
    "        else: self.original_docs = original_docs\n",
    "        # set k1\n",
    "        self.k1 = k1\n",
    "        # set b\n",
    "        self.b = b\n",
    "        # set name\n",
    "        self.name = name\n",
    "        # set full_data\n",
    "        self.full_data = full_data\n",
    "\n",
    "    def __str__(self)->str:\n",
    "        '''\n",
    "        Prints a readable name of the search engine\n",
    "\n",
    "        Output:\n",
    "            str: name of the instance\n",
    "        '''\n",
    "        return(self.name)\n",
    "\n",
    "    def bulk_load(self,data:dict,full_data:pd.DataFrame=None)->None:\n",
    "        '''\n",
    "        Bulk loads new documents to the search engine.\n",
    "\n",
    "        Input:\n",
    "            data: dict, the formatted data\n",
    "            full_data: pd.DataFrame, the data with the full info\n",
    "        '''\n",
    "        # get the original size of the docs\n",
    "        original_len = len(self.docs.keys())\n",
    "        # for each index in the data\n",
    "        for ind in data.keys():\n",
    "            content = data[ind] # quote text\n",
    "            # add to original docs\n",
    "            self.original_docs[ind]=content\n",
    "            # normalize content & add to docs\n",
    "            n_content = normalize_string(str(content))\n",
    "            self.docs[ind]=n_content\n",
    "\n",
    "            # now we want to created the inverted index based on words\n",
    "            words = n_content.split(\" \")\n",
    "            for w in words:\n",
    "                self.index[w][ind]+=1 # update count of word per index\n",
    "        # get new length\n",
    "        new_len = len(self.docs.keys())\n",
    "        print(f'We added {new_len-original_len} documents. The engine now has {new_len} documents.')\n",
    "\n",
    "    def individual_load(self, document:str)-> None:\n",
    "        '''\n",
    "        Load a single document into the search engine. Ideally this should not be used.\n",
    "\n",
    "        Input:\n",
    "            document: str, the new text document to add to the search engine.\n",
    "        '''\n",
    "        # assign new id\n",
    "        new_id = len(self.docs.keys())\n",
    "        # add to docs & original docs\n",
    "        self.original_docs[new_id]=document\n",
    "        n_docs = normalize_string(document)\n",
    "        self.docs[new_id]=n_docs\n",
    "        # now we need to update the inverted index\n",
    "        words = n_docs.split(\" \")\n",
    "        for w in words:\n",
    "            self.index[w][new_id]\n",
    "        print(f'Added document \"{document}\" to search engine.')\n",
    "\n",
    "    def num_docs(self)->int:\n",
    "        '''\n",
    "        Returns the number of docs\n",
    "\n",
    "        Output:\n",
    "            int: length of docs\n",
    "        '''\n",
    "        return len(self.docs.keys())\n",
    "\n",
    "    def find_ids(self, keyword:str)->dict:\n",
    "        '''\n",
    "        Find the doc ids that contain a keyword.\n",
    "\n",
    "        Input:\n",
    "            keyword: str, the word to search\n",
    "        Returns:\n",
    "            dict: keys are the indices and the values are\n",
    "                the frequency of the word in the document\n",
    "        '''\n",
    "        key = normalize_string(keyword)\n",
    "        return(self.index[key])\n",
    "\n",
    "    def bw_idf(self,keyword:str)-> float:\n",
    "        '''\n",
    "        Find the inverse document frequency for a term\n",
    "\n",
    "        Input:\n",
    "            keyword: str, word to search\n",
    "\n",
    "        Output:\n",
    "            float: the idf score\n",
    "        '''\n",
    "        num_docs = self.num_docs()\n",
    "        keyword = normalize_string(keyword)\n",
    "        n_kw = len(self.find_ids(keyword))\n",
    "        idf = log((num_docs-n_kw+0.5)/(n_kw+0.5)+1)\n",
    "        return(idf)\n",
    "\n",
    "    def bm25(self,keyword:str)-> dict[str, float]:\n",
    "        '''\n",
    "        Calculate the bm25 score for every document\n",
    "\n",
    "        Input:\n",
    "            keyword: str, word to search\n",
    "\n",
    "        Output:\n",
    "            dict[str, float]: dict of doc ids & the bm25 score\n",
    "        '''\n",
    "        result = {} # instantiate the output\n",
    "        keyword = normalize_string(keyword)\n",
    "        idf = self.bw_idf(keyword) # get the idf score\n",
    "        # get the avg len of a document\n",
    "        avg_ql = sum(len(d) for d in self.docs.values()) / len(self.docs)\n",
    "        # calculate the bw score for each\n",
    "        for id, freq in self.find_ids(keyword).items(): # for doc id & word freq\n",
    "            numerator = freq*(self.k1+1)\n",
    "            denominator = freq+self.k1*(1 - self.b + self.b * len(self.docs[id]) / avg_ql)\n",
    "            result[id]=idf*numerator / denominator\n",
    "        # return dict with the ids & scores\n",
    "        return result\n",
    "\n",
    "    def bw_search(self,query:str,limit:int=None,context:bool=False)->dict[str,float]:\n",
    "        '''\n",
    "        Completes the bm25 search of the documents using the query and returns\n",
    "\n",
    "        Input:\n",
    "            query: str, the query to search through the documents\n",
    "            limit: int, limits the number of documents\n",
    "            context: bool, if true return also the bm-scores of the lines before & after the selected lines.\n",
    "                There must exist a limit for this to be true.\n",
    "\n",
    "        Output:\n",
    "            dict[str,float]: the index and the bm25 score\n",
    "        '''\n",
    "        # split the query & normalize it\n",
    "        kws = normalize_string(query).split(\" \")\n",
    "        scores = {} # initialize output\n",
    "        for k in kws:\n",
    "            kw_score = self.bm25(k) # get the scores for this word\n",
    "            scores = update_url_scores(scores,kw_score) # add the dict values together\n",
    "        # sort the scores by the bm25 score\n",
    "        sorted_scores = sorted(scores.items(), key=lambda kv: (kv[1], kv[0]),reverse=True)\n",
    "        output = sorted_scores.copy()\n",
    "        # limit the score output\n",
    "        if limit is not None:\n",
    "            output = sorted_scores[:limit]\n",
    "            # check if we need to get context\n",
    "            if context:\n",
    "                # get the top x indices\n",
    "                top_ind = [x[0] for x in output]\n",
    "                # get the context indices (lines before & after)\n",
    "                ctx_ind = [y-1 for y in top_ind]+[y+1 for y in top_ind]\n",
    "                # get their bm_25\n",
    "                more_bm25 = [(z, scores[z]) if z in scores.keys() else (z,0) for z in ctx_ind]\n",
    "                # add to output\n",
    "                output = output+more_bm25\n",
    "        return(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHOULD I ADD STRUCTURED BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We added 144211 documents. The engine now has 144211 documents.\n"
     ]
    }
   ],
   "source": [
    "# create instance of search engine\n",
    "bm25_engine = search_engine(name='BM25 Engine',full_data=complete)\n",
    "# load data in bulk\n",
    "bm25_engine.bulk_load(complete[['quote']].to_dict()['quote'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now I have a search engine! Let's do a test query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_results = bm25_engine.bw_search('dabo',20,context=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(38743, 14.950076796302916),\n",
       " (12663, 12.64571859691301),\n",
       " (61458, 12.50535557934936),\n",
       " (61353, 12.50535557934936),\n",
       " (58731, 12.50535557934936),\n",
       " (53295, 12.50535557934936),\n",
       " (46327, 12.50535557934936),\n",
       " (46320, 12.50535557934936),\n",
       " (38752, 12.50535557934936),\n",
       " (33055, 12.50535557934936),\n",
       " (32506, 12.50535557934936),\n",
       " (32500, 12.50535557934936),\n",
       " (25198, 12.50535557934936),\n",
       " (25187, 12.50535557934936),\n",
       " (23268, 12.50535557934936),\n",
       " (17130, 12.50535557934936),\n",
       " (11519, 12.50535557934936),\n",
       " (1980, 12.50535557934936),\n",
       " (23269, 11.785538827271429),\n",
       " (11518, 11.59258591664298),\n",
       " (38742, 0),\n",
       " (12662, 0),\n",
       " (61457, 0),\n",
       " (61352, 0),\n",
       " (58730, 0),\n",
       " (53294, 5.201571163523684),\n",
       " (46326, 0),\n",
       " (46319, 0),\n",
       " (38751, 0),\n",
       " (33054, 0),\n",
       " (32505, 0),\n",
       " (32499, 0),\n",
       " (25197, 0),\n",
       " (25186, 0),\n",
       " (23267, 0),\n",
       " (17129, 0),\n",
       " (11518, 11.59258591664298),\n",
       " (1979, 0),\n",
       " (23268, 12.50535557934936),\n",
       " (11517, 0),\n",
       " (38744, 0),\n",
       " (12664, 0),\n",
       " (61459, 0),\n",
       " (61354, 0),\n",
       " (58732, 0),\n",
       " (53296, 0),\n",
       " (46328, 0),\n",
       " (46321, 0),\n",
       " (38753, 0),\n",
       " (33056, 0),\n",
       " (32507, 0),\n",
       " (32501, 0),\n",
       " (25199, 0),\n",
       " (25188, 0),\n",
       " (23269, 11.785538827271429),\n",
       " (17131, 0),\n",
       " (11520, 0),\n",
       " (1981, 0),\n",
       " (23270, 0),\n",
       " (11519, 12.50535557934936)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
