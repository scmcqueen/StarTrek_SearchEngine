{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Engine\n",
    "\n",
    "In this file, we walk through the process of creating a search engine with an inverted index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from path import Path\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from math import log\n",
    "import string\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>character</th>\n",
       "      <th>quote</th>\n",
       "      <th>scene</th>\n",
       "      <th>location</th>\n",
       "      <th>view</th>\n",
       "      <th>episode</th>\n",
       "      <th>date</th>\n",
       "      <th>series</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32197</th>\n",
       "      <td>417</td>\n",
       "      <td>O'BRIEN</td>\n",
       "      <td>Then, do it, dammit!</td>\n",
       "      <td>70   INT. OPS</td>\n",
       "      <td>OPS</td>\n",
       "      <td>INT.</td>\n",
       "      <td>If Wishes Were Horses</td>\n",
       "      <td>1993-02-24</td>\n",
       "      <td>Deep Space Nine</td>\n",
       "      <td>416.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136636</th>\n",
       "      <td>307</td>\n",
       "      <td>GEORDI</td>\n",
       "      <td>If there's even one chance in a million I'm ri...</td>\n",
       "      <td>47   INT. OBS LOUNGE</td>\n",
       "      <td>OBS LOUNGE</td>\n",
       "      <td>INT.</td>\n",
       "      <td>Interface</td>\n",
       "      <td>1993-07-14</td>\n",
       "      <td>The Next Generation</td>\n",
       "      <td>255.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77282</th>\n",
       "      <td>387</td>\n",
       "      <td>RIKER</td>\n",
       "      <td>Data, what if we forced an EPS discharge throu...</td>\n",
       "      <td>54   INT. BRIDGE</td>\n",
       "      <td>BRIDGE</td>\n",
       "      <td>INT.</td>\n",
       "      <td>Force of Nature</td>\n",
       "      <td>1993-09-17</td>\n",
       "      <td>The Next Generation</td>\n",
       "      <td>261.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86707</th>\n",
       "      <td>51</td>\n",
       "      <td>TASHA</td>\n",
       "      <td>Aikido. One.</td>\n",
       "      <td>11   INT. HOLODECK - FULL SHOT - OPTICAL (SPLI...</td>\n",
       "      <td>HOLODECK - FULL SHOT - OPTICAL</td>\n",
       "      <td>INT.</td>\n",
       "      <td>Code of Honor</td>\n",
       "      <td>1987-07-02</td>\n",
       "      <td>The Next Generation</td>\n",
       "      <td>104.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103786</th>\n",
       "      <td>227</td>\n",
       "      <td>FALSE PICARD</td>\n",
       "      <td>I have been preoccupied. I've been thinking ab...</td>\n",
       "      <td>31   INT. CAPTAIN'S QUARTERS (OPTICAL)</td>\n",
       "      <td>CAPTAIN'S QUARTERS</td>\n",
       "      <td>INT.</td>\n",
       "      <td>Allegiance</td>\n",
       "      <td>1990-01-15</td>\n",
       "      <td>The Next Generation</td>\n",
       "      <td>166.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        index     character  \\\n",
       "32197     417       O'BRIEN   \n",
       "136636    307        GEORDI   \n",
       "77282     387         RIKER   \n",
       "86707      51         TASHA   \n",
       "103786    227  FALSE PICARD   \n",
       "\n",
       "                                                    quote  \\\n",
       "32197                                Then, do it, dammit!   \n",
       "136636  If there's even one chance in a million I'm ri...   \n",
       "77282   Data, what if we forced an EPS discharge throu...   \n",
       "86707                                        Aikido. One.   \n",
       "103786  I have been preoccupied. I've been thinking ab...   \n",
       "\n",
       "                                                    scene  \\\n",
       "32197                                       70   INT. OPS   \n",
       "136636                               47   INT. OBS LOUNGE   \n",
       "77282                                    54   INT. BRIDGE   \n",
       "86707   11   INT. HOLODECK - FULL SHOT - OPTICAL (SPLI...   \n",
       "103786             31   INT. CAPTAIN'S QUARTERS (OPTICAL)   \n",
       "\n",
       "                              location  view                episode  \\\n",
       "32197                              OPS  INT.  If Wishes Were Horses   \n",
       "136636                      OBS LOUNGE  INT.              Interface   \n",
       "77282                           BRIDGE  INT.        Force of Nature   \n",
       "86707   HOLODECK - FULL SHOT - OPTICAL  INT.          Code of Honor   \n",
       "103786              CAPTAIN'S QUARTERS  INT.             Allegiance   \n",
       "\n",
       "              date               series     file  \n",
       "32197   1993-02-24      Deep Space Nine  416.txt  \n",
       "136636  1993-07-14  The Next Generation  255.txt  \n",
       "77282   1993-09-17  The Next Generation  261.txt  \n",
       "86707   1987-07-02  The Next Generation  104.txt  \n",
       "103786  1990-01-15  The Next Generation  166.txt  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start by loading in the data\n",
    "complete = pd.read_csv('https://scmcqueen.github.io/StarTrekScriptData/complete_data.csv')\n",
    "# Rename columns\n",
    "complete.columns = ['index', 'character', 'quote', 'scene', 'location', 'view',\n",
    "       'episode', 'date', 'series', 'file']\n",
    "# Clean up Character & Quote\n",
    "complete['character'] = complete['character'].apply(lambda text: \" \".join(str(text).split()))\n",
    "complete['quote']=complete['quote'].apply(lambda text: \" \".join(text.split()))\n",
    "# Show sample of data\n",
    "complete.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: POTENTIALLY FIX CHARACTER NAME TOO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data loaded in, we may want to define some functions that we will use when creating the search engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_string(input_string: str) -> str:\n",
    "    '''This function processes a string by removing punctuation,\n",
    "    making text lowercase, and getting rid of extra spaces\n",
    "\n",
    "    For example:\n",
    "        \"Hello,  HI!!! How are     you?\"\n",
    "    becomes\n",
    "        \"hello hi how are you\"\n",
    "    '''\n",
    "    translation_table = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "    string_without_punc = input_string.translate(translation_table)\n",
    "    string_without_double_spaces = ' '.join(string_without_punc.split())\n",
    "    return string_without_double_spaces.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAY WANT TO ALSO LEMMATIZE & DROP STOP WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_url_scores(old: dict[str, float], new: dict[str, float]):\n",
    "    '''This function adds two dictionaries together'''\n",
    "    for url, score in new.items():\n",
    "        if url in old:\n",
    "            old[url] += score\n",
    "        else:\n",
    "            old[url] = score\n",
    "    return old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a Search Engine object with an inverted index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class search_engine:\n",
    "    '''This class creates a search engine object'''\n",
    "    def __init__(self, index:dict[str, dict[str, int]]=None, docs: dict[str, str]=None,\n",
    "        original_docs: dict[str, str]=None,k1:float=1.5,b:float=0.75,\n",
    "        name:str='Default Search Engine',full_data: pd.DataFrame=None):\n",
    "        '''\n",
    "        Instantiate an instance of the search engine class.\n",
    "\n",
    "        Input:\n",
    "            index: dict[str, dict[int,int]], the inverted index\n",
    "            docs: dict[int, str], key is the id of the quote and value is the quote text\n",
    "            original_docs:\n",
    "            k1: float, k1 constant to use for bm25\n",
    "            b: float, b constant to use for bm25\n",
    "            name: string, name used for the search engine instance\n",
    "            full_data:  pd.DataFrame, the original dataset used\n",
    "\n",
    "        Output:\n",
    "            search engine!\n",
    "        '''\n",
    "        # set index\n",
    "        if index is None: self.index = defaultdict(lambda: defaultdict(int))\n",
    "        else: self.index = index\n",
    "        # set docs\n",
    "        if docs is None: self.docs = {}\n",
    "        else: self.docs = docs\n",
    "        # set original docs\n",
    "        if original_docs is None: self.original_docs = {}\n",
    "        else: self.original_docs = original_docs\n",
    "        # set k1\n",
    "        self.k1 = k1\n",
    "        # set b\n",
    "        self.b = b\n",
    "        # set name\n",
    "        self.name = name\n",
    "        # set full_data\n",
    "        self.full_data = full_data\n",
    "\n",
    "    def __str__(self)->str:\n",
    "        '''\n",
    "        Prints a readable name of the search engine\n",
    "\n",
    "        Output:\n",
    "            str: name of the instance\n",
    "        '''\n",
    "        return(self.name)\n",
    "\n",
    "    def bulk_load(self,data:dict,full_data:pd.DataFrame=None)->None:\n",
    "        '''\n",
    "        Bulk loads new documents to the search engine.\n",
    "\n",
    "        Input:\n",
    "            data: dict, the formatted data\n",
    "            full_data: pd.DataFrame, the data with the full info\n",
    "        '''\n",
    "        # get the original size of the docs\n",
    "        original_len = len(self.docs.keys())\n",
    "        # for each index in the data\n",
    "        for ind in data.keys():\n",
    "            content = data[ind] # quote text\n",
    "            # add to original docs\n",
    "            self.original_docs[ind]=content\n",
    "            # normalize content & add to docs\n",
    "            n_content = normalize_string(str(content))\n",
    "            self.docs[ind]=n_content\n",
    "\n",
    "            # now we want to created the inverted index based on words\n",
    "            words = n_content.split(\" \")\n",
    "            for w in words:\n",
    "                self.index[w][ind]+=1 # update count of word per index\n",
    "        # get new length\n",
    "        new_len = len(self.docs.keys())\n",
    "        print(f'We added {new_len-original_len} documents. The engine now has {new_len} documents.')\n",
    "\n",
    "    def individual_load(self, document:str)-> None:\n",
    "        '''\n",
    "        Load a single document into the search engine. Ideally this should not be used.\n",
    "\n",
    "        Input:\n",
    "            document: str, the new text document to add to the search engine.\n",
    "        '''\n",
    "        # assign new id\n",
    "        new_id = len(self.docs.keys())\n",
    "        # add to docs & original docs\n",
    "        self.original_docs[new_id]=document\n",
    "        n_docs = normalize_string(document)\n",
    "        self.docs[new_id]=n_docs\n",
    "        # now we need to update the inverted index\n",
    "        words = n_docs.split(\" \")\n",
    "        for w in words:\n",
    "            self.index[w][new_id]\n",
    "        print(f'Added document \"{document}\" to search engine.')\n",
    "\n",
    "    def num_docs(self)->int:\n",
    "        '''\n",
    "        Returns the number of docs\n",
    "\n",
    "        Output:\n",
    "            int: length of docs\n",
    "        '''\n",
    "        return len(self.docs.keys())\n",
    "\n",
    "    def find_ids(self, keyword:str)->dict:\n",
    "        '''\n",
    "        Find the doc ids that contain a keyword.\n",
    "\n",
    "        Input:\n",
    "            keyword: str, the word to search\n",
    "        Returns:\n",
    "            dict: keys are the indices and the values are\n",
    "                the frequency of the word in the document\n",
    "        '''\n",
    "        key = normalize_string(keyword)\n",
    "        return(self.index[key])\n",
    "\n",
    "    def bw_idf(self,keyword:str)-> float:\n",
    "        '''\n",
    "        Find the inverse document frequency for a term\n",
    "\n",
    "        Input:\n",
    "            keyword: str, word to search\n",
    "\n",
    "        Output:\n",
    "            float: the idf score\n",
    "        '''\n",
    "        num_docs = self.num_docs()\n",
    "        keyword = normalize_string(keyword)\n",
    "        n_kw = len(self.find_ids(keyword))\n",
    "        idf = log((num_docs-n_kw+0.5)/(n_kw+0.5)+1)\n",
    "        return(idf)\n",
    "\n",
    "    def bm25(self,keyword:str)-> dict[str, float]:\n",
    "        '''\n",
    "        Calculate the bm25 score for every document\n",
    "\n",
    "        Input:\n",
    "            keyword: str, word to search\n",
    "\n",
    "        Output:\n",
    "            dict[str, float]: dict of doc ids & the bm25 score\n",
    "        '''\n",
    "        result = {} # instantiate the output\n",
    "        keyword = normalize_string(keyword)\n",
    "        idf = self.bw_idf(keyword) # get the idf score\n",
    "        # get the avg len of a document\n",
    "        avg_ql = sum(len(d) for d in self.docs.values()) / len(self.docs)\n",
    "        # calculate the bw score for each\n",
    "        for id, freq in self.find_ids(keyword).items(): # for doc id & word freq\n",
    "            numerator = freq*(self.k1+1)\n",
    "            denominator = freq+self.k1*(1 - self.b + self.b * len(self.docs[id]) / avg_ql)\n",
    "            result[id]=idf*numerator / denominator\n",
    "        # return dict with the ids & scores\n",
    "        return result\n",
    "\n",
    "    def bw_search(self,query:str,limit:int=20)->dict[str,float]:\n",
    "        '''\n",
    "        Completes the bm25 search of the documents using the query and returns\n",
    "\n",
    "        Input:\n",
    "            query: str, the query to search through the documents\n",
    "            limit: int, limits the number of documents\n",
    "\n",
    "        Output:\n",
    "            dict[str,float]: the index and the bm25 score\n",
    "        '''\n",
    "        # split the query & normalize it\n",
    "        kws = normalize_string(query).split(\" \")\n",
    "        scores = {} # initialize output\n",
    "        for k in kws:\n",
    "            kw_score = self.bm25(k) # get the scores for this word\n",
    "            scores = update_url_scores(scores,kw_score) # add the dict values together\n",
    "        # sort the scores by the bm25 score\n",
    "        sorted_scores = sorted(scores.items(), key=lambda kv: (kv[1], kv[0]),reverse=True)\n",
    "        # limit the score output\n",
    "        limit_scores = sorted_scores[:limit]\n",
    "        return(limit_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_engine = search_engine(name='BM25 Engine',full_data=complete)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
