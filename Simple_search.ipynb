{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from path import Path\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from math import log\n",
    "import string\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Following along with [80 Line Search Engine](https://www.alexmolas.com/2024/02/05/a-search-engine-in-80-lines.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open complete csv\n",
    "complete = pd.read_csv('../StarTrekNextGenScriptData/complete_data.csv')\n",
    "complete.columns = ['index', 'character', 'quote', 'scene', 'location', 'view',\n",
    "       'episode', 'date', 'series', 'file']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Search Engine Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to use  --- may wanna update this to lemmatize\n",
    "def normalize_string(input_string: str) -> str:\n",
    "    translation_table = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "    string_without_punc = input_string.translate(translation_table)\n",
    "    string_without_double_spaces = ' '.join(string_without_punc.split())\n",
    "    return string_without_double_spaces.lower()\n",
    "\n",
    "def update_url_scores(old: dict[str, float], new: dict[str, float]):\n",
    "    for url, score in new.items():\n",
    "        if url in old:\n",
    "            old[url] += score\n",
    "        else:\n",
    "            old[url] = score\n",
    "    return old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class search_engine:\n",
    "    '''Search engine class'''\n",
    "    def __init__(self, index:dict[str, dict[str, int]]=None, docs: dict[str, str]=None,original_docs: dict[str, str]=None,k1:float=1.5,b:float=0.75,name:str='Default Search Engine'):\n",
    "        '''Instantiate the search engine class.\n",
    "        Input:\n",
    "            index: dict[str, dict[int,int]], the inverted index\n",
    "            docs: dict[int, str], key is the id of the quote and value is the quote text\n",
    "            k1: float, k1 constant to use for bm25\n",
    "            b: float, b constant to use for bm25\n",
    "        Output:\n",
    "            None, class instantiation\n",
    "        '''\n",
    "        if index is None:\n",
    "            self.index = defaultdict(lambda: defaultdict(int))\n",
    "        else:\n",
    "            self.index = index\n",
    "        if docs is None:\n",
    "            self.docs = {}\n",
    "        else:\n",
    "            self.docs = docs\n",
    "        if original_docs is None:\n",
    "            self.original_docs = {}\n",
    "        else:\n",
    "            self.original_docs = original_docs\n",
    "        self.k1 =k1\n",
    "        self.b = b\n",
    "        self.name = name\n",
    "\n",
    "    def __str__(self)->str:\n",
    "        '''Print readable name of the search engine\n",
    "        Output:\n",
    "            str: name of the instance\n",
    "        '''\n",
    "        return(self.name)\n",
    "\n",
    "    def bulk_load(self,data:dict)->None:\n",
    "        '''Bulk loads new documents to add to the search engine.\n",
    "\n",
    "        Input:\n",
    "            data: dict[int,str], where int is the id and str is the content\n",
    "        '''\n",
    "        original_len = len(self.docs.keys())\n",
    "        for ind in data.keys():\n",
    "            # get the content & the id\n",
    "            try:\n",
    "                content = data[ind]\n",
    "                \n",
    "                id = ind\n",
    "                # add original docs\n",
    "                self.original_docs[id]=content\n",
    "                # add to doc list\n",
    "                self.docs[id]=normalize_string(str(content))\n",
    "                # normalize content\n",
    "                words = normalize_string(content).split(\" \")\n",
    "            except:\n",
    "                print(content)\n",
    "                print(ind)\n",
    "            # update the index\n",
    "            for word in words:\n",
    "                self.index[word][id] += 1\n",
    "        new_len =original_len = len(self.docs.keys())\n",
    "        print(f'We have added {new_len-original_len} documents. The engine now has {new_len} documents.')\n",
    "\n",
    "    def load(self,document:str)->None:\n",
    "        '''Load a single document into the search engine. Ideally this should not be used.\n",
    "\n",
    "        Input:\n",
    "            document: str, the new text document to add to the search engine.\n",
    "        '''\n",
    "        new_id = len(self.docs.keys())\n",
    "        self.docs[new_id]=normalize_string(document)\n",
    "        words = normalize_string(document).split(\" \")\n",
    "        for word in words:\n",
    "            self.index[word][new_id]\n",
    "\n",
    "    def num_docs(self)->int:\n",
    "        '''Returns the number of docs\n",
    "\n",
    "        Output:\n",
    "            int: length of docs\n",
    "        '''\n",
    "        return len(self.docs.keys())\n",
    "\n",
    "    def find_ids(self,keyword:string)->dict:\n",
    "        keyword =normalize_string(keyword)\n",
    "        return self.index[keyword]\n",
    "\n",
    "    def bw_idf(self,keyword:str)->float:\n",
    "    # for each term, get the idf\n",
    "        num_docs = self.num_docs()\n",
    "        n_kw = len(self.find_ids(keyword))\n",
    "        return log((num_docs-n_kw+0.5)/(n_kw+0.5)+1)\n",
    "    \n",
    "    def bm25(self, keyword:str)-> dict[str, float]:\n",
    "        result = {}\n",
    "        idf = self.bw_idf(keyword)\n",
    "        avg_ql = sum(len(d) for d in self.docs.values()) / len(self.docs)\n",
    "        for id, freq in self.find_ids(keyword).items():\n",
    "            num = freq * (self.k1+1)\n",
    "            denom = freq+self.k1*(1 - self.b + self.b * len(self.docs[id]) / avg_ql)\n",
    "            result[id]=idf * num /denom\n",
    "        return result\n",
    "    \n",
    "    def bw_search(self,query:str,limit:int=None)->dict[str,float]:\n",
    "        kws = normalize_string(query).split(\" \")\n",
    "        scores ={} # dict[str, float] \n",
    "        for k in kws:\n",
    "            kw_url_score = self.bm25(k)\n",
    "            scores = update_url_scores(scores,kw_url_score)\n",
    "        sort_scores = sorted(scores.items(), key=lambda kv: (kv[1], kv[0]),reverse=True)\n",
    "        if limit is not None:\n",
    "            sort_scores = sort_scores[:limit]\n",
    "        return sort_scores\n",
    "    \n",
    "    def pretty_print(self,ids:list)->list:\n",
    "        '''Prints the quotes instead of a list of ids'''\n",
    "        # get context\n",
    "        output = []\n",
    "        for x in ids:\n",
    "            context = [self.original_docs[x-1],self.original_docs[x],self.original_docs[x+1]]\n",
    "            output.append(context)\n",
    "        return(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have added 0 documents. The engine now has 144211 documents.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "144211"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_engine = search_engine()\n",
    "test_engine.bulk_load(complete[['quote']].to_dict()['quote'])\n",
    "test_engine.num_docs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['        Nerys!',\n",
       " \"        Dabo!        That is dabo, isn't it?\",\n",
       " \"   I don't see why you insist on    playing this ridiculous game.\"]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_results = test_engine.bw_search('dabo',20)\n",
    "# sorted_items = sorted(q_results.items(), key=lambda kv: (kv[1], kv[0]),reverse=True)\n",
    "# sorted_items\n",
    "x= test_engine.pretty_print([x[0] for x in q_results])\n",
    "x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', 'N', 'e', 'r', 'y', 's', '!']\n"
     ]
    }
   ],
   "source": [
    "print( [y for y in x[1][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nerys!'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'\\s+', ' ', x[1][0].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps:\n",
    "\n",
    "Now that I feel comfortable with the base idea of how I can implement a search engine, I need to fine tune it. \n",
    "\n",
    "1) pre-process text data much better \n",
    "* I should BOW & vectorize the quotes. Then I can also use the other data points (line in episode, speaker, who is spoken to etc). \n",
    "* From the textbook, it seems like BOW outperforms n-grams, but I should also remove stop words & lemmatize them.\n",
    "* This should allow for similarity vector space recommendation models & some other fancy things: e.g. checking if search term is name of character or episode, etc.\n",
    "2) better display \n",
    "* display non-processed text. Get rid of double spaces. Include the speaker, episode, etc. \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "si649f23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
