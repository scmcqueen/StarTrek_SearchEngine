{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Sentence Embeddings\n",
                "\n",
                "Run this code to get the sentence embeddings for the script data. The file is too large for github, so you can generate it here."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sentence_transformers import SentenceTransformer\n",
                "import pandas as pd\n",
                "import re\n",
                "import numpy as np"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "[\"   You know, Morn -- there's nothing    quite as invigorating as breakfast    in a bar. Where else can you get    raw slug livers first thing in the    morning?\",\n",
                            " \"   What's this?\",\n",
                            " '   What do you mean, \"what\\'s this?\"    It\\'s puree of beetle.',\n",
                            " \"   I didn't order it.\",\n",
                            " '   Of course you \"didn\\'t order it\" --    you don\\'t need to order it. You    have it after work every morning.']"
                        ]
                    },
                    "execution_count": 3,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# open dataframe\n",
                "complete = pd.read_csv('https://scmcqueen.github.io/StarTrekScriptData/complete_data.csv')\n",
                "\n",
                "# get sentences\n",
                "sentences = list(complete['quote'])\n",
                "\n",
                "# preview\n",
                "sentences[:5]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load my girl roberta\n",
                "model = SentenceTransformer('sentence-transformers/all-distilroberta-v1')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "embeddings = model.encode(sentences)\n",
                "# takes 4 minutes to run"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>0</th>\n",
                            "      <th>1</th>\n",
                            "      <th>2</th>\n",
                            "      <th>3</th>\n",
                            "      <th>4</th>\n",
                            "      <th>5</th>\n",
                            "      <th>6</th>\n",
                            "      <th>7</th>\n",
                            "      <th>8</th>\n",
                            "      <th>9</th>\n",
                            "      <th>...</th>\n",
                            "      <th>758</th>\n",
                            "      <th>759</th>\n",
                            "      <th>760</th>\n",
                            "      <th>761</th>\n",
                            "      <th>762</th>\n",
                            "      <th>763</th>\n",
                            "      <th>764</th>\n",
                            "      <th>765</th>\n",
                            "      <th>766</th>\n",
                            "      <th>767</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>-0.014900</td>\n",
                            "      <td>-0.071977</td>\n",
                            "      <td>-0.009478</td>\n",
                            "      <td>-0.029031</td>\n",
                            "      <td>0.014188</td>\n",
                            "      <td>0.016394</td>\n",
                            "      <td>0.027681</td>\n",
                            "      <td>-0.037291</td>\n",
                            "      <td>-0.006358</td>\n",
                            "      <td>-0.004826</td>\n",
                            "      <td>...</td>\n",
                            "      <td>0.001110</td>\n",
                            "      <td>0.012414</td>\n",
                            "      <td>0.030341</td>\n",
                            "      <td>-0.003418</td>\n",
                            "      <td>-0.008822</td>\n",
                            "      <td>0.033325</td>\n",
                            "      <td>-0.001755</td>\n",
                            "      <td>0.068690</td>\n",
                            "      <td>0.020250</td>\n",
                            "      <td>0.039753</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>-0.007164</td>\n",
                            "      <td>-0.013978</td>\n",
                            "      <td>-0.013652</td>\n",
                            "      <td>0.003309</td>\n",
                            "      <td>0.034851</td>\n",
                            "      <td>0.060882</td>\n",
                            "      <td>0.048588</td>\n",
                            "      <td>0.017505</td>\n",
                            "      <td>0.007778</td>\n",
                            "      <td>-0.010418</td>\n",
                            "      <td>...</td>\n",
                            "      <td>0.022243</td>\n",
                            "      <td>0.049582</td>\n",
                            "      <td>-0.008420</td>\n",
                            "      <td>-0.030691</td>\n",
                            "      <td>-0.000645</td>\n",
                            "      <td>-0.050140</td>\n",
                            "      <td>0.083585</td>\n",
                            "      <td>-0.046941</td>\n",
                            "      <td>-0.020030</td>\n",
                            "      <td>0.012742</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>-0.049198</td>\n",
                            "      <td>-0.035306</td>\n",
                            "      <td>0.008143</td>\n",
                            "      <td>0.088346</td>\n",
                            "      <td>0.093932</td>\n",
                            "      <td>0.025349</td>\n",
                            "      <td>-0.010510</td>\n",
                            "      <td>-0.015539</td>\n",
                            "      <td>-0.002362</td>\n",
                            "      <td>-0.013975</td>\n",
                            "      <td>...</td>\n",
                            "      <td>0.003785</td>\n",
                            "      <td>-0.010067</td>\n",
                            "      <td>0.007732</td>\n",
                            "      <td>-0.049809</td>\n",
                            "      <td>0.051560</td>\n",
                            "      <td>0.024296</td>\n",
                            "      <td>0.035107</td>\n",
                            "      <td>-0.066092</td>\n",
                            "      <td>-0.020333</td>\n",
                            "      <td>-0.009799</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>-0.043590</td>\n",
                            "      <td>-0.021896</td>\n",
                            "      <td>-0.019405</td>\n",
                            "      <td>0.027361</td>\n",
                            "      <td>-0.020507</td>\n",
                            "      <td>0.035430</td>\n",
                            "      <td>-0.040713</td>\n",
                            "      <td>0.043625</td>\n",
                            "      <td>0.065613</td>\n",
                            "      <td>0.002796</td>\n",
                            "      <td>...</td>\n",
                            "      <td>0.009547</td>\n",
                            "      <td>0.010854</td>\n",
                            "      <td>-0.027765</td>\n",
                            "      <td>0.049275</td>\n",
                            "      <td>0.023973</td>\n",
                            "      <td>0.004445</td>\n",
                            "      <td>0.072360</td>\n",
                            "      <td>-0.029266</td>\n",
                            "      <td>-0.031327</td>\n",
                            "      <td>0.004318</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>-0.053526</td>\n",
                            "      <td>0.001219</td>\n",
                            "      <td>0.001101</td>\n",
                            "      <td>0.031380</td>\n",
                            "      <td>-0.008443</td>\n",
                            "      <td>0.052395</td>\n",
                            "      <td>-0.043175</td>\n",
                            "      <td>-0.038514</td>\n",
                            "      <td>-0.012206</td>\n",
                            "      <td>0.025206</td>\n",
                            "      <td>...</td>\n",
                            "      <td>-0.040711</td>\n",
                            "      <td>0.010254</td>\n",
                            "      <td>0.007623</td>\n",
                            "      <td>0.042096</td>\n",
                            "      <td>-0.003285</td>\n",
                            "      <td>0.002070</td>\n",
                            "      <td>0.003425</td>\n",
                            "      <td>-0.044013</td>\n",
                            "      <td>0.021388</td>\n",
                            "      <td>0.001928</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "<p>5 rows × 768 columns</p>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "        0         1         2         3         4         5         6    \\\n",
                            "0 -0.014900 -0.071977 -0.009478 -0.029031  0.014188  0.016394  0.027681   \n",
                            "1 -0.007164 -0.013978 -0.013652  0.003309  0.034851  0.060882  0.048588   \n",
                            "2 -0.049198 -0.035306  0.008143  0.088346  0.093932  0.025349 -0.010510   \n",
                            "3 -0.043590 -0.021896 -0.019405  0.027361 -0.020507  0.035430 -0.040713   \n",
                            "4 -0.053526  0.001219  0.001101  0.031380 -0.008443  0.052395 -0.043175   \n",
                            "\n",
                            "        7         8         9    ...       758       759       760       761  \\\n",
                            "0 -0.037291 -0.006358 -0.004826  ...  0.001110  0.012414  0.030341 -0.003418   \n",
                            "1  0.017505  0.007778 -0.010418  ...  0.022243  0.049582 -0.008420 -0.030691   \n",
                            "2 -0.015539 -0.002362 -0.013975  ...  0.003785 -0.010067  0.007732 -0.049809   \n",
                            "3  0.043625  0.065613  0.002796  ...  0.009547  0.010854 -0.027765  0.049275   \n",
                            "4 -0.038514 -0.012206  0.025206  ... -0.040711  0.010254  0.007623  0.042096   \n",
                            "\n",
                            "        762       763       764       765       766       767  \n",
                            "0 -0.008822  0.033325 -0.001755  0.068690  0.020250  0.039753  \n",
                            "1 -0.000645 -0.050140  0.083585 -0.046941 -0.020030  0.012742  \n",
                            "2  0.051560  0.024296  0.035107 -0.066092 -0.020333 -0.009799  \n",
                            "3  0.023973  0.004445  0.072360 -0.029266 -0.031327  0.004318  \n",
                            "4 -0.003285  0.002070  0.003425 -0.044013  0.021388  0.001928  \n",
                            "\n",
                            "[5 rows x 768 columns]"
                        ]
                    },
                    "execution_count": 8,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "embedding_df = pd.DataFrame(embeddings)\n",
                "# embedding_df.to_csv('st_embeddings.csv')\n",
                "embedding_df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Sentence Embeddings\n",
                "\n",
                "Run this code to get the sentiment for the script data. This takes a long time to run, so I saved a copy as complete_sentiment.csv."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import nltk\n",
                "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
                "import spacy\n",
                "import pandas as pd\n",
                "from spacy.tokens import Doc\n",
                "import csv"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sent_analyzer = SentimentIntensityAnalyzer()\n",
                "def sentiment_scores(docx):\n",
                "    global sent_analyzer\n",
                "    return sent_analyzer.polarity_scores(docx.text)\n",
                "\n",
                "def sentiment_analysis(input:str):\n",
                "    global sent_analyzer\n",
                "    # set the sentiment analysis functions\n",
                "    nlp = spacy.load(\"en_core_web_sm\")\n",
                "    return(nlp(input)._.sentimenter['compound'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "# sentiment analysis\n",
                "Doc.set_extension(\"sentimenter\",getter=sentiment_scores)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "0.4572"
                        ]
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# demonstrate function\n",
                "sentiment_analysis('Data science is so cool')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load the data & neaten it up\n",
                "complete = pd.read_csv('https://scmcqueen.github.io/StarTrekScriptData/complete_data.csv').reset_index()\n",
                "complete.columns = ['index','line','character', 'quote', 'scene', 'location', 'view',\n",
                "       'episode', 'date', 'series', 'file']\n",
                "complete['quote']=complete['quote'].apply(lambda text: \" \".join(text.split()))\n",
                "complete['character']=complete['character'].fillna('NA')\n",
                "complete['character']=complete['character'].apply(lambda x: x.replace('\"','').replace('\\t',''))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "I use a sentiment mapping dictionary because it takes a long time to run the sentiment analysis. This way, if the code has to stop for any reason there is still a way to save the data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [],
            "source": [
                "# open the current sentiment map\n",
                "with open(\"sent_map.csv\", mode=\"r\") as file:\n",
                "    reader = csv.DictReader(file)\n",
                "    sent_data = [row for row in reader][0]  # List of dictionaries\n",
                "\n",
                "sent_data={int(k):float(v) for k,v in sent_data.items()}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "100\n",
                        "200\n",
                        "300\n",
                        "400\n"
                    ]
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[24], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# get the quote & sentiment\u001b[39;00m\n\u001b[1;32m      8\u001b[0m quote \u001b[38;5;241m=\u001b[39m complete\u001b[38;5;241m.\u001b[39miloc[x][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquote\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 9\u001b[0m q_sent \u001b[38;5;241m=\u001b[39m sentiment_analysis(quote)\n\u001b[1;32m     10\u001b[0m sent_data[x]\u001b[38;5;241m=\u001b[39mq_sent\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# if we did 100, save it\u001b[39;00m\n",
                        "Cell \u001b[0;32mIn[4], line 9\u001b[0m, in \u001b[0;36msentiment_analysis\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m sent_analyzer\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# set the sentiment analysis functions\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m(nlp(\u001b[38;5;28minput\u001b[39m)\u001b[38;5;241m.\u001b[39m_\u001b[38;5;241m.\u001b[39msentimenter[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompound\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
                        "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/spacy/__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[1;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[1;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m util\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[1;32m     52\u001b[0m         name,\n\u001b[1;32m     53\u001b[0m         vocab\u001b[38;5;241m=\u001b[39mvocab,\n\u001b[1;32m     54\u001b[0m         disable\u001b[38;5;241m=\u001b[39mdisable,\n\u001b[1;32m     55\u001b[0m         enable\u001b[38;5;241m=\u001b[39menable,\n\u001b[1;32m     56\u001b[0m         exclude\u001b[38;5;241m=\u001b[39mexclude,\n\u001b[1;32m     57\u001b[0m         config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m     58\u001b[0m     )\n",
                        "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/spacy/util.py:465\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_lang_class(name\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblank:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))()\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_package(name):  \u001b[38;5;66;03m# installed as package\u001b[39;00m\n\u001b[0;32m--> 465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m load_model_from_package(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Path(name)\u001b[38;5;241m.\u001b[39mexists():  \u001b[38;5;66;03m# path to model data directory\u001b[39;00m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m load_model_from_path(Path(name), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
                        "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/spacy/util.py:501\u001b[0m, in \u001b[0;36mload_model_from_package\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load a model from an installed package.\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \n\u001b[1;32m    486\u001b[0m \u001b[38;5;124;03mname (str): The package name.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;124;03mRETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(name)\n\u001b[0;32m--> 501\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mload(vocab\u001b[38;5;241m=\u001b[39mvocab, disable\u001b[38;5;241m=\u001b[39mdisable, enable\u001b[38;5;241m=\u001b[39menable, exclude\u001b[38;5;241m=\u001b[39mexclude, config\u001b[38;5;241m=\u001b[39mconfig)\n",
                        "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/en_core_web_sm/__init__.py:10\u001b[0m, in \u001b[0;36mload\u001b[0;34m(**overrides)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moverrides):\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m load_model_from_init_py(\u001b[38;5;18m__file__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moverrides)\n",
                        "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/spacy/util.py:682\u001b[0m, in \u001b[0;36mload_model_from_init_py\u001b[0;34m(init_file, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m    681\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE052\u001b[38;5;241m.\u001b[39mformat(path\u001b[38;5;241m=\u001b[39mdata_path))\n\u001b[0;32m--> 682\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m load_model_from_path(\n\u001b[1;32m    683\u001b[0m     data_path,\n\u001b[1;32m    684\u001b[0m     vocab\u001b[38;5;241m=\u001b[39mvocab,\n\u001b[1;32m    685\u001b[0m     meta\u001b[38;5;241m=\u001b[39mmeta,\n\u001b[1;32m    686\u001b[0m     disable\u001b[38;5;241m=\u001b[39mdisable,\n\u001b[1;32m    687\u001b[0m     enable\u001b[38;5;241m=\u001b[39menable,\n\u001b[1;32m    688\u001b[0m     exclude\u001b[38;5;241m=\u001b[39mexclude,\n\u001b[1;32m    689\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m    690\u001b[0m )\n",
                        "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/spacy/util.py:539\u001b[0m, in \u001b[0;36mload_model_from_path\u001b[0;34m(model_path, meta, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    537\u001b[0m overrides \u001b[38;5;241m=\u001b[39m dict_to_dot(config, for_overrides\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    538\u001b[0m config \u001b[38;5;241m=\u001b[39m load_config(config_path, overrides\u001b[38;5;241m=\u001b[39moverrides)\n\u001b[0;32m--> 539\u001b[0m nlp \u001b[38;5;241m=\u001b[39m load_model_from_config(\n\u001b[1;32m    540\u001b[0m     config,\n\u001b[1;32m    541\u001b[0m     vocab\u001b[38;5;241m=\u001b[39mvocab,\n\u001b[1;32m    542\u001b[0m     disable\u001b[38;5;241m=\u001b[39mdisable,\n\u001b[1;32m    543\u001b[0m     enable\u001b[38;5;241m=\u001b[39menable,\n\u001b[1;32m    544\u001b[0m     exclude\u001b[38;5;241m=\u001b[39mexclude,\n\u001b[1;32m    545\u001b[0m     meta\u001b[38;5;241m=\u001b[39mmeta,\n\u001b[1;32m    546\u001b[0m )\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nlp\u001b[38;5;241m.\u001b[39mfrom_disk(model_path, exclude\u001b[38;5;241m=\u001b[39mexclude, overrides\u001b[38;5;241m=\u001b[39moverrides)\n",
                        "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/spacy/util.py:587\u001b[0m, in \u001b[0;36mload_model_from_config\u001b[0;34m(config, meta, vocab, disable, enable, exclude, auto_fill, validate)\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[38;5;66;03m# This will automatically handle all codes registered via the languages\u001b[39;00m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;66;03m# registry, including custom subclasses provided via entry points\u001b[39;00m\n\u001b[1;32m    586\u001b[0m lang_cls \u001b[38;5;241m=\u001b[39m get_lang_class(nlp_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlang\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 587\u001b[0m nlp \u001b[38;5;241m=\u001b[39m lang_cls\u001b[38;5;241m.\u001b[39mfrom_config(\n\u001b[1;32m    588\u001b[0m     config,\n\u001b[1;32m    589\u001b[0m     vocab\u001b[38;5;241m=\u001b[39mvocab,\n\u001b[1;32m    590\u001b[0m     disable\u001b[38;5;241m=\u001b[39mdisable,\n\u001b[1;32m    591\u001b[0m     enable\u001b[38;5;241m=\u001b[39menable,\n\u001b[1;32m    592\u001b[0m     exclude\u001b[38;5;241m=\u001b[39mexclude,\n\u001b[1;32m    593\u001b[0m     auto_fill\u001b[38;5;241m=\u001b[39mauto_fill,\n\u001b[1;32m    594\u001b[0m     validate\u001b[38;5;241m=\u001b[39mvalidate,\n\u001b[1;32m    595\u001b[0m     meta\u001b[38;5;241m=\u001b[39mmeta,\n\u001b[1;32m    596\u001b[0m )\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nlp\n",
                        "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/spacy/language.py:1855\u001b[0m, in \u001b[0;36mLanguage.from_config\u001b[0;34m(cls, config, vocab, disable, enable, exclude, meta, auto_fill, validate)\u001b[0m\n\u001b[1;32m   1849\u001b[0m warn_if_jupyter_cupy()\n\u001b[1;32m   1851\u001b[0m \u001b[38;5;66;03m# Note that we don't load vectors here, instead they get loaded explicitly\u001b[39;00m\n\u001b[1;32m   1852\u001b[0m \u001b[38;5;66;03m# inside stuff like the spacy train function. If we loaded them here,\u001b[39;00m\n\u001b[1;32m   1853\u001b[0m \u001b[38;5;66;03m# then we would load them twice at runtime: once when we make from config,\u001b[39;00m\n\u001b[1;32m   1854\u001b[0m \u001b[38;5;66;03m# and then again when we load from disk.\u001b[39;00m\n\u001b[0;32m-> 1855\u001b[0m nlp \u001b[38;5;241m=\u001b[39m lang_cls(\n\u001b[1;32m   1856\u001b[0m     vocab\u001b[38;5;241m=\u001b[39mvocab,\n\u001b[1;32m   1857\u001b[0m     create_tokenizer\u001b[38;5;241m=\u001b[39mcreate_tokenizer,\n\u001b[1;32m   1858\u001b[0m     create_vectors\u001b[38;5;241m=\u001b[39mcreate_vectors,\n\u001b[1;32m   1859\u001b[0m     meta\u001b[38;5;241m=\u001b[39mmeta,\n\u001b[1;32m   1860\u001b[0m )\n\u001b[1;32m   1861\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m after_creation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1862\u001b[0m     nlp \u001b[38;5;241m=\u001b[39m after_creation(nlp)\n",
                        "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/spacy/language.py:220\u001b[0m, in \u001b[0;36mLanguage.__init__\u001b[0;34m(self, vocab, max_length, meta, create_tokenizer, create_vectors, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    218\u001b[0m     tokenizer_cfg \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnlp\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m    219\u001b[0m     create_tokenizer \u001b[38;5;241m=\u001b[39m registry\u001b[38;5;241m.\u001b[39mresolve(tokenizer_cfg)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 220\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m create_tokenizer(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m=\u001b[39m batch_size\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_error_handler \u001b[38;5;241m=\u001b[39m raise_error\n",
                        "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/spacy/language.py:117\u001b[0m, in \u001b[0;36mcreate_tokenizer.<locals>.tokenizer_factory\u001b[0;34m(nlp)\u001b[0m\n\u001b[1;32m    115\u001b[0m suffix_search \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mcompile_suffix_regex(suffixes)\u001b[38;5;241m.\u001b[39msearch \u001b[38;5;28;01mif\u001b[39;00m suffixes \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m infix_finditer \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mcompile_infix_regex(infixes)\u001b[38;5;241m.\u001b[39mfinditer \u001b[38;5;28;01mif\u001b[39;00m infixes \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Tokenizer(\n\u001b[1;32m    118\u001b[0m     nlp\u001b[38;5;241m.\u001b[39mvocab,\n\u001b[1;32m    119\u001b[0m     rules\u001b[38;5;241m=\u001b[39mnlp\u001b[38;5;241m.\u001b[39mDefaults\u001b[38;5;241m.\u001b[39mtokenizer_exceptions,\n\u001b[1;32m    120\u001b[0m     prefix_search\u001b[38;5;241m=\u001b[39mprefix_search,\n\u001b[1;32m    121\u001b[0m     suffix_search\u001b[38;5;241m=\u001b[39msuffix_search,\n\u001b[1;32m    122\u001b[0m     infix_finditer\u001b[38;5;241m=\u001b[39minfix_finditer,\n\u001b[1;32m    123\u001b[0m     token_match\u001b[38;5;241m=\u001b[39mnlp\u001b[38;5;241m.\u001b[39mDefaults\u001b[38;5;241m.\u001b[39mtoken_match,\n\u001b[1;32m    124\u001b[0m     url_match\u001b[38;5;241m=\u001b[39mnlp\u001b[38;5;241m.\u001b[39mDefaults\u001b[38;5;241m.\u001b[39murl_match,\n\u001b[1;32m    125\u001b[0m )\n",
                        "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/spacy/tokenizer.pyx:71\u001b[0m, in \u001b[0;36mspacy.tokenizer.Tokenizer.__init__\u001b[0;34m()\u001b[0m\n",
                        "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/spacy/tokenizer.pyx:578\u001b[0m, in \u001b[0;36mspacy.tokenizer.Tokenizer._load_special_cases\u001b[0;34m()\u001b[0m\n",
                        "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/spacy/tokenizer.pyx:622\u001b[0m, in \u001b[0;36mspacy.tokenizer.Tokenizer.add_special_case\u001b[0;34m()\u001b[0m\n",
                        "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/spacy/tokenizer.pyx:172\u001b[0m, in \u001b[0;36mspacy.tokenizer.Tokenizer._tokenize_affixes\u001b[0;34m()\u001b[0m\n",
                        "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/spacy/tokens/doc.pyx:254\u001b[0m, in \u001b[0;36mspacy.tokens.doc.Doc.__init__\u001b[0;34m()\u001b[0m\n",
                        "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/spacy/tokens/_dict_proxies.py:26\u001b[0m, in \u001b[0;36mSpanGroups.__init__\u001b[0;34m(self, doc, items)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A dict-like proxy held by the Doc, to control access to span groups.\"\"\"\u001b[39;00m\n\u001b[1;32m     24\u001b[0m _EMPTY_BYTES \u001b[38;5;241m=\u001b[39m srsly\u001b[38;5;241m.\u001b[39mmsgpack_dumps([])\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mself\u001b[39m, doc: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDoc\u001b[39m\u001b[38;5;124m\"\u001b[39m, items: Iterable[Tuple[\u001b[38;5;28mstr\u001b[39m, SpanGroup]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m()\n\u001b[1;32m     28\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdoc_ref \u001b[38;5;241m=\u001b[39m weakref\u001b[38;5;241m.\u001b[39mref(doc)\n\u001b[1;32m     30\u001b[0m     UserDict\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, items)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "source": [
                "# for each index, check if it is in the sentiment map. If not, calculate it\n",
                "for x in list(complete['index']):\n",
                "    # check if it is already mapped\n",
                "    if x in sent_data.keys():\n",
                "        continue\n",
                "\n",
                "    # get the quote & sentiment\n",
                "    quote = complete.iloc[x]['quote']\n",
                "    q_sent = sentiment_analysis(quote)\n",
                "    sent_data[x]=q_sent\n",
                "\n",
                "    # if we did 100, save it\n",
                "    if x%100==0:\n",
                "        print(x)\n",
                "        with open(\"sent_map.csv\", \"w\", newline=\"\") as f:\n",
                "            w = csv.DictWriter(f, sent_data.keys())\n",
                "            w.writeheader()\n",
                "            w.writerow(sent_data)\n",
                "\n",
                "# once we are done, save it\n",
                "with open(\"sent_map.csv\", \"w\", newline=\"\") as f:\n",
                "            w = csv.DictWriter(f, sent_data.keys())\n",
                "            w.writeheader()\n",
                "            w.writerow(sent_data)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# save the sentiment data with the full data\n",
                "complete['sentiment']=complete['index'].apply(lambda x: sent_data[x])\n",
                "complete.to_csv('complete_sentiment.csv')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "base",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
