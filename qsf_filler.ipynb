{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to Write Qualtrics Surveys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import simple_search_func as ss\n",
    "import re\n",
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load up the qsf!\n",
    "#with open('Query_Template.qsf', 'r') as file:\n",
    "with open('Query_Template.qsf', 'r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choices_dict = {\n",
    "    'love': ['hello', 'hi', 'are you', 'darling', 'sweetheart', 'honey', 'romance', 'affection', 'passion', 'devotion', \n",
    "             'adore', 'cherish', 'fondness', 'embrace', 'beloved', 'cuddle', 'valentine', 'kiss', 'heart', 'soulmate'],\n",
    "    'sadness': ['meow', 'meow meow', 'meowwww', 'tears', 'gloom', 'despair', 'melancholy', 'sorrow', 'blue', 'grief', \n",
    "                'heartache', 'mourn', 'loneliness', 'misery', 'pain', 'regret', 'weep', 'hurt', 'lost', 'downcast'],\n",
    "    'kiss': ['smoocher', 'kombucher', 'peck', 'smooch', 'liplock', 'mwah', 'pucker', 'osculation', 'romantic', 'passionate',\n",
    "             'sweet', 'gentle', 'soft', 'warm', 'butterfly', 'cheek', 'forehead', 'eskimo', 'lovebite', 'chaste'],\n",
    "    'happiness': ['joy', 'smile', 'laughter', 'cheer', 'delight', 'bliss', 'glee', 'ecstatic', 'merry', 'euphoria', \n",
    "                  'sunny', 'content', 'radiant', 'jubilant', 'bubbly', 'grateful', 'optimistic', 'carefree', 'vivacious', 'buoyant'],\n",
    "    'anger': ['rage', 'fury', 'wrath', 'outrage', 'resentment', 'bitterness', 'spite', 'grudge', 'irritation', 'annoyance',\n",
    "              'exasperation', 'hostility', 'hatred', 'indignation', 'fuming', 'seething', 'mad', 'infuriated', 'irate', 'tempest'],\n",
    "    'fear': ['terror', 'panic', 'dread', 'fright', 'horror', 'alarm', 'anxiety', 'nervous', 'uneasy', 'apprehension',\n",
    "             'timid', 'startle', 'tremble', 'phobia', 'shiver', 'quiver', 'spooked', 'petrified', 'chill', 'paranoia'],\n",
    "    'surprise': ['shock', 'amazement', 'astonishment', 'wonder', 'startled', 'stunned', 'flabbergasted', 'aghast', 'dumbfounded', 'jolted',\n",
    "                 'marvel', 'unexpected', 'perplexed', 'bewildered', 'gobsmacked', 'astounded', 'overwhelmed', 'speechless', 'baffled', 'staggered'],\n",
    "    'friendship': ['bond', 'companion', 'ally', 'trust', 'loyal', 'camaraderie', 'mate', 'bro', 'sisterhood', 'brotherhood',\n",
    "                   'affection', 'partnership', 'support', 'understanding', 'team', 'respect', 'connect', 'harmony', 'together', 'kindred'],\n",
    "    'jealousy': ['envy', 'resent', 'covet', 'green-eyed', 'possessive', 'suspicion', 'mistrust', 'distrust', 'bitterness', 'spite',\n",
    "                 'comparison', 'insecurity', 'longing', 'competitiveness', 'inferiority', 'grudge', 'fuming', 'anger', 'watchful', 'wary'],\n",
    "    'nostalgia': ['memories', 'past', 'bittersweet', 'longing', 'reminisce', 'yearning', 'flashback', 'old days', 'sentimental', 'melancholy',\n",
    "                  'childhood', 'throwback', 'wistful', 'retro', 'antique', 'classic', 'golden days', 'vintage', 'heartwarming', 'timeless'],\n",
    "    'fear2': ['terror', 'panic', 'dread', 'fright', 'horror', 'alarm', 'anxiety', 'nervous', 'uneasy', 'apprehension',\n",
    "             'timid', 'startle', 'tremble', 'phobia', 'shiver', 'quiver', 'spooked', 'petrified', 'chill', 'paranoia'],\n",
    "    'surprise2': ['shock', 'amazement', 'astonishment', 'wonder', 'startled', 'stunned', 'flabbergasted', 'aghast', 'dumbfounded', 'jolted',\n",
    "                 'marvel', 'unexpected', 'perplexed', 'bewildered', 'gobsmacked', 'astounded', 'overwhelmed', 'speechless', 'baffled', 'staggered'],\n",
    "    'friendship2': ['bond', 'companion', 'ally', 'trust', 'loyal', 'camaraderie', 'mate', 'bro', 'sisterhood', 'brotherhood',\n",
    "                   'affection', 'partnership', 'support', 'understanding', 'team', 'respect', 'connect', 'harmony', 'together', 'kindred'],\n",
    "    'jealousy2': ['envy', 'resent', 'covet', 'green-eyed', 'possessive', 'suspicion', 'mistrust', 'distrust', 'bitterness', 'spite',\n",
    "                 'comparison', 'insecurity', 'longing', 'competitiveness', 'inferiority', 'grudge', 'fuming', 'anger', 'watchful', 'wary'],\n",
    "    'nostalgia2': ['memories', 'past', 'bittersweet', 'longing', 'reminisce', 'yearning', 'flashback', 'old days', 'sentimental', 'melancholy',\n",
    "                  'childhood', 'throwback', 'wistful', 'retro', 'antique', 'classic', 'golden days', 'vintage', 'heartwarming', 'timeless']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = list(choices_dict.keys())# will be dict keys\n",
    "counter = 0\n",
    "next_topic=None\n",
    "\n",
    "\n",
    "# iterate\n",
    "for x in data['SurveyElements']:\n",
    "    if type(x['Payload']) != dict:\n",
    "        continue\n",
    "    if 'QuestionType' not in x['Payload'].keys():\n",
    "        continue\n",
    "    if x['Payload']['QuestionType']=='MC': # group rank, influences others\n",
    "        text = topics[counter]\n",
    "        old_text = x['SecondaryAttribute']\n",
    "        new_q = f'Select whether or not the following quotes are similar to the query \"{text}\"'\n",
    "        x['Payload']['QuestionDescription']= new_q\n",
    "        x['SecondaryAttribute'] = new_q\n",
    "        x['Payload']['QuestionText']= new_q\n",
    "        choices = choices_dict[text]\n",
    "        new_choices = {str(x+1):{'Display': choices[x]} for x in range(len(choices))}\n",
    "        x['Payload']['Choices']=new_choices\n",
    "        x['Payload']['ChoiceOrder']=list(new_choices.keys())\n",
    "        counter +=1\n",
    "counter = 0\n",
    "for x in data['SurveyElements']:\n",
    "    if type(x['Payload']) != dict:\n",
    "        continue\n",
    "    if 'QuestionType' not in x['Payload'].keys():\n",
    "        continue\n",
    "    if x['Payload']['QuestionType']=='RO': # rank only, depends on other\n",
    "        # THIS COMES FIRST\n",
    "        # update Secondary Attribute: replace whatever is in \"\"\n",
    "        text = topics[counter]\n",
    "        next_topic = text\n",
    "        old_text = x['SecondaryAttribute']\n",
    "        new_q = f'Rank these quotes on how funny they are!'\n",
    "        x['SecondaryAttribute'] = new_q\n",
    "        # update ['Payload']['QuestionText'] replace whatever is in \"\"\n",
    "        x['Payload']['QuestionText']= new_q\n",
    "        x['Payload']['QuestionDescription']= new_q\n",
    "        counter+=1\n",
    "        #counter +=1\n",
    "\n",
    "# save\n",
    "with open('test.qsf', 'w') as json_file:\n",
    "    json.dump(data, json_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make this a function\n",
    "\n",
    "def new_survey_file(choices_dict:dict,file_name:str,template_name:str='Query_Template.qsf'):\n",
    "    with open(template_name, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    topics = list(choices_dict.keys())# will be dict keys\n",
    "    c_topics = topics.copy()\n",
    "    # surveyid = 'newstring'\n",
    "\n",
    "    # iterate\n",
    "    for x in data['SurveyElements']:\n",
    "        if type(x['Payload']) != dict:\n",
    "            continue\n",
    "        if 'QuestionType' not in x['Payload'].keys():\n",
    "            continue\n",
    "        # x['SurveyID']= surveyid\n",
    "        if x['Payload']['QuestionType']=='MC': # group rank, influences others\n",
    "            text = topics.pop()\n",
    "            new_q = f'Select whether or not the following quotes are similar to the query \"{text}\"'\n",
    "            x['Payload']['QuestionDescription']= new_q\n",
    "            x['SecondaryAttribute'] = new_q\n",
    "            x['Payload']['QuestionText']= new_q\n",
    "            choices = choices_dict[text]\n",
    "            new_choices = {str(x+1):{'Display': choices[x]} for x in range(len(choices))}\n",
    "            x['Payload']['Choices']=new_choices\n",
    "            x['Payload']['ChoiceOrder']=list(new_choices.keys())\n",
    "        if x['Payload']['QuestionType']=='RO': # rank only, depends on other\n",
    "            # update Secondary Attribute: replace whatever is in \"\"\n",
    "            text = c_topics.pop()\n",
    "            new_q = f'Rank these quotes on how funny they are!'\n",
    "            x['SecondaryAttribute'] = new_q\n",
    "            # update ['Payload']['QuestionText'] replace whatever is in \"\"\n",
    "            x['Payload']['QuestionText']= new_q\n",
    "            x['Payload']['QuestionDescription']= new_q\n",
    "\n",
    "    # save\n",
    "    with open(file_name, 'w') as json_file:\n",
    "        json.dump(data, json_file) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load up examples\n",
    "\n",
    "Now that I have the function written, I need to load up the search engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have added 0 documents. The engine now has 144211 documents.\n",
      "Complete data added.\n"
     ]
    }
   ],
   "source": [
    "# load up that search\n",
    "complete = pd.read_csv('https://scmcqueen.github.io/StarTrekScriptData/complete_data.csv')\n",
    "complete.columns = ['index','character', 'quote', 'scene', 'location', 'view',\n",
    "       'episode', 'date', 'series', 'file']\n",
    "complete['quote']=complete['quote'].apply(lambda text: \" \".join(text.split()))\n",
    "complete['character']=complete['character'].fillna('NA')\n",
    "test_engine = ss.search_engine()\n",
    "test_engine.bulk_load(complete[['quote']].to_dict()['quote'])\n",
    "test_engine.add_df(complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=test_engine.bw_search('candle',30)\n",
    "search_results = test_engine.pretty_print([x[0] for x in results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [x[0] for x in results]\n",
    "choices = [' '.join(x) for x in search_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_1 =  ['candle','dream','doctor','bajor','prophets','inequality','struggle','engineer','prune','humanity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = []\n",
    "choices = []\n",
    "search_term = []\n",
    "file_input = {}\n",
    "\n",
    "for term in keywords_1:\n",
    "    # do the search\n",
    "    results=test_engine.bw_search(term,20)\n",
    "    search_results = test_engine.pretty_print([x[0] for x in results])\n",
    "    # update values\n",
    "    t_ind = [x[0] for x in results]\n",
    "    t_choices = [' '.join(x) for x in search_results]\n",
    "    t_search = [term]*len(t_ind)\n",
    "    # update big list\n",
    "    indices+=t_ind\n",
    "    choices+=t_choices\n",
    "    search_term+=t_search\n",
    "    # update the dict\n",
    "    file_input[term] = t_choices  \n",
    "\n",
    "new_survey_file(choices_dict=file_input,file_name='Evaluation/day_1.qsf',template_name='Query_Template.qsf')\n",
    "pd.DataFrame({'indices':indices,'choices':choices,'search':search_term}).to_csv('Evaluation/day_1_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_1 =  ['candle','dream','doctor','bajor','prophets','inequality','struggle','engineer','prune','humanity',\n",
    "               'story', 'religion', 'child', 'molly','counselor']\n",
    "keywords_2 =  ['archaeology','chateau','trial','fiction','romance','wedding','father','kahless','chocolate','future',\n",
    "               'bridge','saucer','coffee','romulans','colony']\n",
    "keywords_3 =  ['raktajino','commit no errors','tanagra','borg','cardassia','poetry','spot','assimilate','unusual','barclay',\n",
    "               'acquisition','rules','vedek','warp crystal',\"bat'leth\"]\n",
    "keywords_4 =  ['blood wine','gagh','cook','celebrate','impossible','earl grey','jake','geordi','riker','poker',\n",
    "               'phaser','occupation','conscious minds','tailor','neutral zone']\n",
    "keywords_5 =  ['men','women','miles','worf','bashir','commander','captain','kira','troi','guinan',\n",
    "               'shields','unification','mister spock','tribble','android']\n",
    "keywords_6 =  ['unraveled','friendship','hailing frequencies','universe','traveler','wesley','good tea','good book','make it so','flute',\n",
    "               'neutrino levels','noonian soong','breen','evidence','vacation']\n",
    "keywords_7 =  ['memory','merry man','obedient','honor','san francisco','new orleans','liberation','imagination','maquis','tribunal',\n",
    "               'trill','lore','risa','detective','ezri']\n",
    "keywords_8 =  ['not picard','ice cream','morn','weyoun','dukat','civilization','replicators','truth','lunch','past',\n",
    "               'empath','betray','warp bubble','jadzia','pregnant']\n",
    "keywords_9 =  ['vineyard','lwaxana','bucket','justice is justice','espionage','same lie','mother','klingon','baseball','moriarty',\n",
    "               'vulcan','moon','lower decks','the academy','full impulse']\n",
    "keywords_10 =  ['enterprise','ferengi','nagus','death','root beer','alpha quadrant','wormhole','tasha','holosuite','holmes',\n",
    "                'the defiant','ensign ro','dabo girl','secret agent','oo-mox']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = {1:keywords_1,\n",
    "            2:keywords_2,\n",
    "            3:keywords_3,\n",
    "            4:keywords_4,\n",
    "            5:keywords_5,\n",
    "            6:keywords_6,\n",
    "            7:keywords_7,\n",
    "            8:keywords_8,\n",
    "            9:keywords_9,\n",
    "            10:keywords_10\n",
    "            }\n",
    "\n",
    "for x in keywords.keys():\n",
    "    counter = str(x)\n",
    "    t_keys = keywords[x]\n",
    "\n",
    "    indices = []\n",
    "    choices = []\n",
    "    search_term = []\n",
    "    file_input = {}\n",
    "\n",
    "    for term in t_keys:\n",
    "        # do the search\n",
    "        results=test_engine.bw_search(term,20)\n",
    "        search_results = test_engine.pretty_print([x[0] for x in results])\n",
    "        # update values\n",
    "        t_ind = [x[0] for x in results]\n",
    "        t_choices = [' '.join(x) for x in search_results]\n",
    "        t_search = [term]*len(t_ind)\n",
    "        # update big list\n",
    "        indices+=t_ind\n",
    "        choices+=t_choices\n",
    "        search_term+=t_search\n",
    "        # update the dict\n",
    "        file_input[term] = t_choices\n",
    "\n",
    "    new_survey_file(choices_dict=file_input,file_name=f'Evaluation/day_{counter}.qsf',template_name='Query_Template.qsf')\n",
    "    pd.DataFrame({'indices':indices,'choices':choices,'search':search_term}).to_csv(f'Evaluation/day_{counter}_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start by reading a file\n",
    "results = pd.read_csv('Evaluation/Results/day1_skyeler_only.tsv',sep='\\t',encoding=\"utf-16\")\n",
    "questions = pd.read_csv('Evaluation/day_1_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['StartDate', 'EndDate', 'Status', 'IPAddress', 'Progress', 'Duration (in seconds)', 'Finished', 'RecordedDate', 'ResponseId', 'RecipientLastName', 'RecipientFirstName', 'RecipientEmail', 'ExternalReference', 'LocationLatitude', 'LocationLongitude', 'DistributionChannel', 'UserLanguage', 'Q1', 'Q2', 'Q4', 'Q6', 'Q8', 'Q10', 'Q12', 'Q14', 'Q16', 'Q18', 'Q20', 'Q22', 'Q24', 'Q26', 'Q28', 'Q30', 'Q3_1', 'Q3_2', 'Q3_3', 'Q3_4', 'Q3_5', 'Q3_6', 'Q3_7', 'Q3_8', 'Q3_9', 'Q3_10', 'Q3_11', 'Q3_12', 'Q3_13', 'Q3_14', 'Q3_15', 'Q3_16', 'Q3_17', 'Q3_18', 'Q3_19', 'Q3_20', 'Q3_21', 'Q3_22', 'Q3_23', 'Q3_24', 'Q3_25', 'Q3_26', 'Q3_27', 'Q3_28', 'Q3_29', 'Q3_30', 'Q5_1', 'Q5_2', 'Q5_3', 'Q5_4', 'Q5_5', 'Q5_6', 'Q5_7', 'Q5_8', 'Q5_9', 'Q5_10', 'Q5_11', 'Q5_12', 'Q5_13', 'Q5_14', 'Q5_15', 'Q5_16', 'Q5_17', 'Q5_18', 'Q5_19', 'Q5_20', 'Q5_21', 'Q5_22', 'Q5_23', 'Q5_24', 'Q5_25', 'Q5_26', 'Q5_27', 'Q5_28', 'Q5_29', 'Q5_30', 'Q7_1', 'Q7_2', 'Q7_3', 'Q7_4', 'Q7_5', 'Q7_6', 'Q7_7', 'Q7_8', 'Q7_9', 'Q7_10', 'Q7_11', 'Q7_12', 'Q7_13', 'Q7_14', 'Q7_15', 'Q7_16', 'Q7_17', 'Q7_18', 'Q7_19', 'Q7_20', 'Q9_1', 'Q9_2', 'Q9_3', 'Q9_4', 'Q9_5', 'Q9_6', 'Q9_7', 'Q9_8', 'Q9_9', 'Q9_10', 'Q9_11', 'Q9_12', 'Q9_13', 'Q9_14', 'Q9_15', 'Q9_16', 'Q9_17', 'Q9_18', 'Q9_19', 'Q9_20', 'Q9_21', 'Q9_22', 'Q9_23', 'Q9_24', 'Q9_25', 'Q9_26', 'Q9_27', 'Q9_28', 'Q9_29', 'Q9_30', 'Q11_1', 'Q11_2', 'Q11_3', 'Q11_4', 'Q11_5', 'Q11_6', 'Q11_7', 'Q11_8', 'Q11_9', 'Q11_10', 'Q11_11', 'Q11_12', 'Q11_13', 'Q11_14', 'Q11_15', 'Q11_16', 'Q11_17', 'Q11_18', 'Q11_19', 'Q11_20', 'Q11_21', 'Q11_22', 'Q11_23', 'Q11_24', 'Q11_25', 'Q11_26', 'Q11_27', 'Q11_28', 'Q11_29', 'Q11_30', 'Q13_1', 'Q13_2', 'Q13_3', 'Q13_4', 'Q13_5', 'Q13_6', 'Q13_7', 'Q13_8', 'Q13_9', 'Q13_10', 'Q13_11', 'Q13_12', 'Q13_13', 'Q13_14', 'Q13_15', 'Q13_16', 'Q13_17', 'Q13_18', 'Q13_19', 'Q13_20', 'Q13_21', 'Q13_22', 'Q13_23', 'Q13_24', 'Q13_25', 'Q13_26', 'Q13_27', 'Q13_28', 'Q13_29', 'Q13_30', 'Q15_1', 'Q15_2', 'Q15_3', 'Q15_4', 'Q15_5', 'Q17_1', 'Q17_2', 'Q17_3', 'Q17_4', 'Q17_5', 'Q17_6', 'Q17_7', 'Q17_8', 'Q17_9', 'Q17_10', 'Q17_11', 'Q17_12', 'Q17_13', 'Q17_14', 'Q17_15', 'Q17_16', 'Q17_17', 'Q17_18', 'Q17_19', 'Q17_20', 'Q17_21', 'Q17_22', 'Q17_23', 'Q17_24', 'Q17_25', 'Q17_26', 'Q17_27', 'Q17_28', 'Q17_29', 'Q17_30', 'Q19_1', 'Q19_2', 'Q19_3', 'Q19_4', 'Q19_5', 'Q19_6', 'Q19_7', 'Q19_8', 'Q19_9', 'Q19_10', 'Q19_11', 'Q19_12', 'Q19_13', 'Q19_14', 'Q19_15', 'Q19_16', 'Q19_17', 'Q19_18', 'Q19_19', 'Q19_20', 'Q19_21', 'Q19_22', 'Q19_23', 'Q19_24', 'Q19_25', 'Q19_26', 'Q19_27', 'Q19_28', 'Q19_29', 'Q19_30', 'Q21_1', 'Q21_2', 'Q21_3', 'Q21_4', 'Q21_5', 'Q21_6', 'Q21_7', 'Q21_8', 'Q21_9', 'Q21_10', 'Q21_11', 'Q21_12', 'Q21_13', 'Q21_14', 'Q21_15', 'Q21_16', 'Q21_17', 'Q23_1', 'Q23_2', 'Q23_3', 'Q23_4', 'Q23_5', 'Q23_6', 'Q23_7', 'Q23_8', 'Q23_9', 'Q23_10', 'Q23_11', 'Q23_12', 'Q23_13', 'Q23_14', 'Q23_15', 'Q23_16', 'Q23_17', 'Q23_18', 'Q23_19', 'Q23_20', 'Q23_21', 'Q23_22', 'Q23_23', 'Q23_24', 'Q23_25', 'Q23_26', 'Q23_27', 'Q23_28', 'Q23_29', 'Q23_30', 'Q25_1', 'Q25_2', 'Q25_3', 'Q25_4', 'Q25_5', 'Q25_6', 'Q25_7', 'Q25_8', 'Q25_9', 'Q25_10', 'Q25_11', 'Q25_12', 'Q25_13', 'Q25_14', 'Q25_15', 'Q25_16', 'Q25_17', 'Q25_18', 'Q25_19', 'Q25_20', 'Q25_21', 'Q25_22', 'Q25_23', 'Q25_24', 'Q25_25', 'Q25_26', 'Q25_27', 'Q25_28', 'Q25_29', 'Q25_30', 'Q29_1', 'Q29_2', 'Q29_3', 'Q29_4', 'Q29_5', 'Q29_6', 'Q29_7', 'Q29_8', 'Q29_9', 'Q29_10', 'Q29_11', 'Q29_12', 'Q29_13', 'Q29_14', 'Q29_15', 'Q29_16', 'Q29_17', 'Q29_18', 'Q29_19', 'Q29_20', 'Q29_21', 'Q29_22', 'Q29_23', 'Q29_24', 'Q29_25', 'Q29_26', 'Q29_27', 'Q29_28', 'Q29_29', 'Q29_30', 'Q31_1', 'Q31_2', 'Q31_3', 'Q31_4', 'Q31_5', 'Q31_6', 'Q31_7', 'Q31_8', 'Q31_9', 'Q31_10', 'Q31_11', 'Q31_12', 'Q31_13', 'Q31_14', 'Q31_15', 'Q31_16', 'Q31_17', 'Q31_18', 'Q31_19', 'Q31_20', 'Q31_21', 'Q31_22', 'Q31_23', 'Q31_24', 'Q31_25', 'Q31_26', 'Q31_27', 'Q31_28', 'Q31_29', 'Q31_30']\n"
     ]
    }
   ],
   "source": [
    "print(list(results.columns))\n",
    "results =results.drop(columns=['StartDate', 'EndDate', 'Status', 'IPAddress', 'Progress', 'Duration (in seconds)', 'Finished', 'RecordedDate', 'ResponseId', 'RecipientLastName', 'RecipientFirstName', 'RecipientEmail', 'ExternalReference', 'LocationLatitude', 'LocationLongitude', 'DistributionChannel', 'UserLanguage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#           MY IDEAL OUTPUT\n",
    "# NAME, QUERY, RANKING 1, RANKING 3, RANKING 3, RANKING 4, RANKING 5, .... 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rank these quotes on how funny they are! - QUARK: Odo? QUARK: Doctor Bashir! Doctor Bashir! BASHIR: Welcome back, Constable.'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['Q3_2'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Ranker                                              Query Ranking\n",
      "0  Skyeler  Rank these quotes on how funny they are! - KIR...       3\n",
      "1  Skyeler  Rank these quotes on how funny they are! - KIR...       2\n",
      "2  Skyeler  Rank these quotes on how funny they are! - BAS...       1\n",
      "3  Skyeler  Rank these quotes on how funny they are! - SIS...       5\n",
      "4  Skyeler  Rank these quotes on how funny they are! - NEC...       4\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file -- this is so bad\n",
    "df = results\n",
    "\n",
    "# Drop the first two metadata rows and reset index\n",
    "df_cleaned = df.iloc[2:].reset_index(drop=True)\n",
    "\n",
    "# Extract the ranker's name (assuming it's in Q1 based on the structure)\n",
    "df_cleaned.rename(columns={\"Q1\": \"Ranker\"}, inplace=True)\n",
    "\n",
    "# Extract ranking-related columns and their associated queries\n",
    "ranking_columns = [col for col in df_cleaned.columns if col.startswith(\"Q31_\")]\n",
    "queries = {col: df.iloc[0][col] for col in ranking_columns}  # Extract query descriptions from row 0\n",
    "\n",
    "# Transform data into long format\n",
    "rankings_list = []\n",
    "for index, row in df_cleaned.iterrows():\n",
    "    ranker = row[\"Ranker\"]\n",
    "    for col in ranking_columns:\n",
    "        ranking = row[col]\n",
    "        if pd.notna(ranking):  # Only keep non-null rankings\n",
    "            rankings_list.append({\"Ranker\": ranker, \"Query\": queries[col], \"Ranking\": ranking})\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_rankings = pd.DataFrame(rankings_list)\n",
    "\n",
    "# Display the first few rows of the structured data\n",
    "print(df_rankings.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "si649f23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
